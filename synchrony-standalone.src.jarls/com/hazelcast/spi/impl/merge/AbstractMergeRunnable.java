package com.hazelcast.spi.impl.merge;

import com.hazelcast.config.InMemoryFormat;
import com.hazelcast.internal.cluster.ClusterService;
import com.hazelcast.internal.config.MergePolicyValidator;
import com.hazelcast.internal.serialization.InternalSerializationService;
import com.hazelcast.logging.ILogger;
import com.hazelcast.nio.serialization.Data;
import com.hazelcast.nio.serialization.DataType;
import com.hazelcast.spi.NodeEngine;
import com.hazelcast.spi.Operation;
import com.hazelcast.spi.OperationFactory;
import com.hazelcast.spi.OperationService;
import com.hazelcast.spi.merge.MergingEntry;
import com.hazelcast.spi.merge.SplitBrainMergePolicy;
import com.hazelcast.spi.partition.IPartitionService;
import com.hazelcast.util.function.BiConsumer;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;

public abstract class AbstractMergeRunnable<K, V, Store, MergingItem extends MergingEntry<K, V>> extends Object implements Runnable {
  private static final long TIMEOUT_FACTOR = 500L;
  
  private static final long MINIMAL_TIMEOUT_MILLIS = TimeUnit.SECONDS.toMillis(5L);
  
  private final Semaphore semaphore;
  
  private final ILogger logger;
  
  private final String serviceName;
  
  private final ClusterService clusterService;
  
  private final OperationService operationService;
  
  private final IPartitionService partitionService;
  
  private final AbstractSplitBrainHandlerService<Store> splitBrainHandlerService;
  
  private final InternalSerializationService serializationService;
  
  private Map<String, Collection<Store>> mergingStoresByName;
  
  protected AbstractMergeRunnable(String serviceName, Collection<Store> mergingStores, AbstractSplitBrainHandlerService<Store> splitBrainHandlerService, NodeEngine nodeEngine) {
    this.semaphore = new Semaphore(0);
    this.mergingStoresByName = groupStoresByName(mergingStores);
    this.serviceName = serviceName;
    this.logger = nodeEngine.getLogger(getClass());
    this.partitionService = nodeEngine.getPartitionService();
    this.clusterService = nodeEngine.getClusterService();
    this.operationService = nodeEngine.getOperationService();
    this.serializationService = (InternalSerializationService)nodeEngine.getSerializationService();
    this.splitBrainHandlerService = splitBrainHandlerService;
  }
  
  private Map<String, Collection<Store>> groupStoresByName(Collection<Store> stores) {
    Map<String, Collection<Store>> storesByName = new HashMap<String, Collection<Store>>();
    for (Store store : stores) {
      String dataStructureName = getDataStructureName(store);
      Collection<Store> storeList = (Collection)storesByName.get(dataStructureName);
      if (storeList == null) {
        storeList = new LinkedList<Store>();
        storesByName.put(dataStructureName, storeList);
      } 
      storeList.add(store);
    } 
    return storesByName;
  }
  
  public final void run() {
    onRunStart();
    int mergedCount = 0;
    mergedCount += mergeWithSplitBrainMergePolicy();
    mergedCount += mergeWithLegacyMergePolicy();
    waitMergeEnd(mergedCount);
  }
  
  protected void onRunStart() {}
  
  private int mergeWithSplitBrainMergePolicy() {
    int mergedCount = 0;
    Iterator<Map.Entry<String, Collection<Store>>> iterator = this.mergingStoresByName.entrySet().iterator();
    while (iterator.hasNext()) {
      Map.Entry<String, Collection<Store>> entry = (Map.Entry)iterator.next();
      String dataStructureName = (String)entry.getKey();
      Collection<Store> stores = (Collection)entry.getValue();
      if (getMergePolicy(dataStructureName) instanceof SplitBrainMergePolicy) {
        MergingItemBiConsumer consumer = newConsumer(dataStructureName);
        for (Store store : stores) {
          try {
            mergeStore(store, consumer);
            MergingItemBiConsumer.access$000(consumer);
          } finally {
            asyncDestroyStores(Collections.singleton(store));
          } 
        } 
        mergedCount += MergingItemBiConsumer.access$100(consumer);
        onMerge(dataStructureName);
        iterator.remove();
      } 
    } 
    return mergedCount;
  }
  
  private MergingItemBiConsumer newConsumer(String dataStructureName) {
    SplitBrainMergePolicy<V, MergingItem> policy = getSplitBrainMergePolicy(dataStructureName);
    int batchSize = getBatchSize(dataStructureName);
    return new MergingItemBiConsumer(this, dataStructureName, policy, batchSize);
  }
  
  private SplitBrainMergePolicy<V, MergingItem> getSplitBrainMergePolicy(String dataStructureName) { return (SplitBrainMergePolicy)getMergePolicy(dataStructureName); }
  
  private int mergeWithLegacyMergePolicy() {
    LegacyOperationBiConsumer consumer = new LegacyOperationBiConsumer(this, null);
    iterator = this.mergingStoresByName.entrySet().iterator();
    while (iterator.hasNext()) {
      try {
        Map.Entry<String, Collection<Store>> entry = (Map.Entry)iterator.next();
        String dataStructureName = (String)entry.getKey();
        Collection<Store> stores = (Collection)entry.getValue();
        if (canMergeLegacy(dataStructureName)) {
          for (Store store : stores) {
            try {
              mergeStoreLegacy(store, consumer);
            } finally {
              asyncDestroyStores(Collections.singleton(store));
            } 
          } 
          onMerge(dataStructureName);
        } else {
          asyncDestroyStores(stores);
        } 
      } finally {
        iterator.remove();
      } 
    } 
    return LegacyOperationBiConsumer.access$300(consumer);
  }
  
  private boolean canMergeLegacy(String dataStructureName) {
    Object mergePolicy = getMergePolicy(dataStructureName);
    InMemoryFormat inMemoryFormat = getInMemoryFormat(dataStructureName);
    return MergePolicyValidator.checkMergePolicySupportsInMemoryFormat(dataStructureName, mergePolicy, inMemoryFormat, false, this.logger);
  }
  
  private void waitMergeEnd(int mergedCount) {
    try {
      long timeoutMillis = Math.max(mergedCount * 500L, MINIMAL_TIMEOUT_MILLIS);
      if (!this.semaphore.tryAcquire(mergedCount, timeoutMillis, TimeUnit.MILLISECONDS))
        this.logger.warning("Split-brain healing didn't finish within the timeout..."); 
    } catch (InterruptedException e) {
      this.logger.finest("Interrupted while waiting for split-brain healing...");
      Thread.currentThread().interrupt();
    } 
  }
  
  protected InternalSerializationService getSerializationService() { return this.serializationService; }
  
  protected Data toData(Object object) { return this.serializationService.toData(object); }
  
  protected Data toHeapData(Object object) { return this.serializationService.toData(object, DataType.HEAP); }
  
  private void asyncDestroyStores(Collection<Store> stores) {
    for (Store store : stores)
      this.splitBrainHandlerService.asyncDestroyStores(Collections.singleton(store), getPartitionId(store)); 
  }
  
  protected void onMerge(String dataStructureName) {}
  
  protected abstract void mergeStore(Store paramStore, BiConsumer<Integer, MergingItem> paramBiConsumer);
  
  protected abstract void mergeStoreLegacy(Store paramStore, BiConsumer<Integer, Operation> paramBiConsumer);
  
  protected abstract int getBatchSize(String paramString);
  
  protected abstract Object getMergePolicy(String paramString);
  
  protected abstract String getDataStructureName(Store paramStore);
  
  protected abstract int getPartitionId(Store paramStore);
  
  protected abstract InMemoryFormat getInMemoryFormat(String paramString);
  
  protected abstract OperationFactory createMergeOperationFactory(String paramString, SplitBrainMergePolicy<V, MergingItem> paramSplitBrainMergePolicy, int[] paramArrayOfInt, List<MergingItem>[] paramArrayOfList);
}
