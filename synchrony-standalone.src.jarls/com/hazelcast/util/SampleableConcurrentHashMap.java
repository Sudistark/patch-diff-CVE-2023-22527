package com.hazelcast.util;

import com.hazelcast.internal.eviction.Expirable;
import com.hazelcast.nio.serialization.SerializableByConvention;
import java.util.AbstractMap;
import java.util.Collections;
import java.util.EnumSet;
import java.util.List;
import java.util.Map;

@SerializableByConvention
public class SampleableConcurrentHashMap<K, V> extends ConcurrentReferenceHashMap<K, V> {
  private static final float LOAD_FACTOR = 0.91F;
  
  public SampleableConcurrentHashMap(int initialCapacity) { this(initialCapacity, ConcurrentReferenceHashMap.ReferenceType.STRONG, ConcurrentReferenceHashMap.ReferenceType.STRONG); }
  
  public SampleableConcurrentHashMap(int initialCapacity, ConcurrentReferenceHashMap.ReferenceType keyType, ConcurrentReferenceHashMap.ReferenceType valueType) { this(initialCapacity, 0.91F, 1, keyType, valueType, null); }
  
  private SampleableConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel, ConcurrentReferenceHashMap.ReferenceType keyType, ConcurrentReferenceHashMap.ReferenceType valueType, EnumSet<ConcurrentReferenceHashMap.Option> options) { super(initialCapacity, loadFactor, concurrencyLevel, keyType, valueType, options); }
  
  public int fetchKeys(int tableIndex, int size, List<K> keys) {
    int nextTableIndex;
    long now = Clock.currentTimeMillis();
    ConcurrentReferenceHashMap.Segment<K, V> segment = this.segments[0];
    ConcurrentReferenceHashMap.HashEntry[] arrayOfHashEntry = segment.table;
    if (tableIndex >= 0 && tableIndex < segment.table.length) {
      nextTableIndex = tableIndex;
    } else {
      nextTableIndex = arrayOfHashEntry.length - 1;
    } 
    int counter = 0;
    while (nextTableIndex >= 0 && counter < size) {
      ConcurrentReferenceHashMap.HashEntry<K, V> nextEntry = arrayOfHashEntry[nextTableIndex--];
      while (nextEntry != null) {
        if (nextEntry.key() != null) {
          V value = (V)nextEntry.value();
          if (isValidForFetching(value, now)) {
            keys.add(nextEntry.key());
            counter++;
          } 
        } 
        nextEntry = nextEntry.next;
      } 
    } 
    return nextTableIndex;
  }
  
  public int fetchEntries(int tableIndex, int size, List<Map.Entry<K, V>> entries) {
    int nextTableIndex;
    long now = Clock.currentTimeMillis();
    ConcurrentReferenceHashMap.Segment<K, V> segment = this.segments[0];
    ConcurrentReferenceHashMap.HashEntry[] arrayOfHashEntry = segment.table;
    if (tableIndex >= 0 && tableIndex < segment.table.length) {
      nextTableIndex = tableIndex;
    } else {
      nextTableIndex = arrayOfHashEntry.length - 1;
    } 
    int counter = 0;
    while (nextTableIndex >= 0 && counter < size) {
      ConcurrentReferenceHashMap.HashEntry<K, V> nextEntry = arrayOfHashEntry[nextTableIndex--];
      while (nextEntry != null) {
        if (nextEntry.key() != null) {
          V value = (V)nextEntry.value();
          if (isValidForFetching(value, now)) {
            K key = (K)nextEntry.key();
            entries.add(new AbstractMap.SimpleEntry(key, value));
            counter++;
          } 
        } 
        nextEntry = nextEntry.next;
      } 
    } 
    return nextTableIndex;
  }
  
  protected boolean isValidForFetching(V value, long now) {
    if (value instanceof Expirable)
      return !((Expirable)value).isExpiredAt(now); 
    return true;
  }
  
  protected <E extends SamplingEntry> E createSamplingEntry(K key, V value) { return (E)new SamplingEntry(key, value); }
  
  public <E extends SamplingEntry> Iterable<E> getRandomSamples(int sampleCount) {
    if (sampleCount < 0)
      throw new IllegalArgumentException("Sample count cannot be a negative value."); 
    if (sampleCount == 0 || size() == 0)
      return Collections.EMPTY_LIST; 
    return new LazySamplingEntryIterableIterator(this, sampleCount, null);
  }
  
  protected boolean isValidForSampling(K key, V value) { return (key != null && value != null); }
}
